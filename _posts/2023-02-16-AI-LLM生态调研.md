---
layout: post
title: AI及LLM生态调研
category: AI
catalog: true
published: false
tags:
  - AI
time: '2023.02.16 10:18:00'
---
> 最近貌似全民都在关注ChatGPT，当前这个领域的生态发展看起来挺快的，对这块没有太多概念，借机学学。

> 所有内容、图片均摘抄自参考文档内容。

# 一、背景介绍
## 神经网络
人脑的神经网络有大概1000亿的神经元，每个神经元大概有1万个关联节点。神经网络的信号传递和周边的神经元的信息传递有关联。

在原来深度学习的基础上，把神经网络做大，当参数规模做到700亿以上时，出现了智能涌现的现象。但随着模型规模的扩大，性能提升的速率会逐渐减缓。

# 二、主流模型
## [OpenAI GPT](https://github.com/openai)
GPT：generatively pre-trained Transformer。  

### 初代GPT
在2018年，OpenAI发布最初版本的GPT模型，从 OpenAI 公开的论文（Improving Language Understanding by Generative Pre-Training）可以了解到，这个模型采用了12层的Transformer Decoder结构，用了大约5GB的无监督文本数据进行语言模型任务的训练。虽然**初代GPT模型采用的就已经是生成式的预训练**（这也是 GPT 名字的由来，Generative Pre-Training，即生成式预训练），但使用的是无监督预训练+下游任务微调的范式。

### GPT-2
在2019年，OpenAI发布了有48层Transformer结构的[GPT-2模型](https://github.com/openai/gpt-2)。在发布的论文(Language Models are Unsupervised Multitask Learners)中，他们发现通过无监督数据配合生成式训练后，GPT展示出了零样本(zero-shot)的多任务能力。

### GPT-3
在2020年，OpenAI发布了1750亿参数量的GPT-3，一个即便以现在的眼光去看也大得让人惊叹的模型。虽然OpenAI没有明确公开训练这个模型的费用，但大家的估算是当时花了 1200 万美元。同时公开的还有一篇长达60多页的论文(Language Models are Few-Shot Learners)。由于GPT-3没有开源，有些研究员训练出了类似GPT-3的模型：[GPT-Neo](https://zenodo.org/record/5297715)、[GPT-J-6B](https://github.com/kingoflolz/mesh-transformer-jax)。

### CodeX
在2021年，OpenAI在对GPT-3的研究中还有一个意外的发现，它能够根据一些注释生成很简单的代码。因此在随后的2021年，他们对生成代码这件事情进行了专门的研究投入，并发布了CodeX模型。它可以看作是一个有着代码专精能力的GPT模型，能够根据自然语言输入生成比较复杂的代码。

### InstructGPT
在2022年，OpenAI 发表了 InstructGPT 的论文(Training language models to follow instructions with human feedback)，从中我们可以一窥解决「一本正经地胡说八道」和「输出有害的内容」这样问题的方法。**InstructGPT 提出了两个阶段的路径来让 GPT 学习人类给出的「优秀范例」，第一阶段是监督学习，第二阶段是强化学习。**
![]({{site.baseurl}}/img/2023/Q2/20230410-GPT.jpg)

### GPT3.5/ChatGPT
根据分析，GPT-3.5 系列的模型有可能并不是在 GPT-3 上继续微调而来，而很可能是将代码和自然语言的数据融合在一起，重新从零开始训练了一个基础模型。这个模型可能比 GPT-3 的 1750 亿参数量更大，它在OpenAI的API中被命名为 codex-davinci-002。然后在这个基础模型上，通过指令微调和人类反馈得到了一系列后续的模型，包括ChatGPT。

### GPT-4
2023年3月，OpenAI公开的一种多模态模型，是对前几个月发布的ChatGPT的多模态升级。GPT-4模型可对图文多模态输入生成应答文字，以及对视觉元素的分类、分析和隐含语义提取，并表现出优秀的应答能力。  
ChatGPT根据输入的指令(prompt)进行回复的能力，是来自于一种被称为指令微调的模型训练方式(prompt tuning)。其实原理很简单，模型依然还是「根据输入的内容，预测下一个token是什么」，只是在指令微调的阶段，输入的内容被换成了这些事先写好的prompt，而prompt后面需要生成的内容，则是事先写好的答案。因此在这一阶段和一开始所说的无监督自回归语言模型训练，最大的不同在于数据。这里的数据，也就是prompt以及对应的回复，都是人写的，换句话说，这一阶段用的是人工标注的数据进行的监督训练。

### [nanoGPT](https://github.com/karpathy/nanoGPT)
由前特斯拉AI总监、李飞飞徒弟-Andrej Karpathy创建。

## [ChatGLM](https://chatglm.cn/blog)
模型回答：由清华大学 KEG 实验室与智谱AI共同训练的大型语言模型，名为“ChatGLM”。ChatGLM 是由清华大学 KEG 实验室与智谱AI共同开发而成。  
有两个开源版本：ChatGLM-6B，ChatGLM-130B。

## [LLaMA](https://github.com/facebookresearch/llama)
meta开源的ChatGPT模型。

### [DocsGPT](https://github.com/arc53/DocsGPT)
帮助大家能更加高效的在项目文档中找到价值信息。


# 二、周边配套服务
## 2.1 [Amazon SageMaker](https://catalog.us-east-1.prod.workshops.aws/workshops/1ac668b1-dbd3-4b45-bf0a-5bc36138fcf1/zh-CN)
SageMaker非常明智的把注意力放到**训练模型**和**发布模型**上， 让数据科学家去做要针对的编程工作。

## 2.2 [Hugging Face](https://huggingface.co/)
Hugging face 起初是一家总部位于纽约的聊天机器人初创服务商，他们本来打算创业做聊天机器人，然后在github上开源了一个Transformers库，虽然聊天机器人业务没搞起来，但是他们的这个库在机器学习社区迅速大火起来。目前已经共享了超100,000个预训练模型，10,000个数据集，变成了机器学习界的github。

## 2.3 [langchain](https://langchain.readthedocs.io/en/latest/)
把LLM模型进行了封装，方便模块化调用。

## 2.4 [chatterbot](https://chatterbot.readthedocs.io/)

# 三、LLM架构逻辑
## 前馈神经网络(FNN)
## 循环神经网络(RNN)
RNN是一种用于处理序列数据的神经网络，和一般的神经网络来说，它能够处理序列变化的数据。比如：我明天离开杭州 和 我明天来杭州。这两句中的杭州代表的意义是不一样的。如果用前馈神经网络就无法将这个场景区分开。

[参考文档](https://zhuanlan.zhihu.com/p/30844905)

### 长短时记忆网络(LSTM)

## 向量表示词
### one-hot
### 词袋模型
### 词嵌入(Word Embedding)

## 卷积网络(CNN)

## Attention
Query(查询/意图)：当前单词的表示；
Key(索引)：输入序列中各个位置的单词对当前单词的重要性；
Value(值)：代表了各个位置上单词的特征和含义；

### Self-Attention

## Transformer
Encoder+Decoder的过程就是一个Transformer。

### Encoder
纯Encoder模型适用于只需要理解输入语义的任务，例如句子分类、命名实体识别。

### Decoder
纯Decoder模型适用于生成式任务，例如文本生成。

### Encoder-Decoder模型 or Seq2seq模型
Encoder-Decoder模型或Seq2Seq模型适用于需要基于输入的生成式任务，例如翻译、摘要。
![](https://github.com/datawhalechina/learn-nlp-with-transformers/raw/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/1-2-translation.gif)

# 四、参考文档
- [人人都能看懂的LSTM](https://zhuanlan.zhihu.com/p/32085405)
- [NLP中的RNN、Seq2Seq与attention注意力机制](https://zhuanlan.zhihu.com/p/52119092)
- [Transofrmers快速入门](https://transformers.run/)
- [基于transformers的自然语言处理(NLP)入门](https://github.com/datawhalechina/learn-nlp-with-transformers)
- [ChatGPT的前世今生：OpenAI的技术「执拗」与「豪赌」](https://www.8btc.com/article/6805740)
- [热点解读：大模型的突现能力和ChatGPT引爆的范式转变](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650864784&idx=3&sn=fb8ad092ad32623af7ea822652cd14cc&chksm=84e538eeb392b1f8ff40fa10dc2c84b56904fed261ff15f97f36a1023f887807af62ea39bde7&scene=21#wechat_redirect)
- [ChatGPT的各项超能力从哪儿来？万字拆解追溯技术路线图来了！](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650864144&idx=4&sn=1270624988d70f44d4059af7ac4ae4e0&chksm=84e53e6eb392b7785418e8257952284cfe6dd801d84404958fb917c461da792039626e172c31&scene=21#wechat_redirect)
- [ChatGLM](https://chatglm.cn/blog)
- [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)
- [如何看待亚马逊最新发布的 SageMaker？](https://www.zhihu.com/question/263394266)
- [Huggingface 超详细介绍](https://zhuanlan.zhihu.com/p/535100411)
- [李飞飞高徒教你从0到1构建GPT，马斯克点赞](https://zhuanlan.zhihu.com/p/600057039)
- [Let's build GPT: from scratch, in code, spelled out.  29:30](https://www.youtube.com/watch?v=kCc8FmEb1nY)
- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
- [Building a GPT](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing)
- [Mu Li讲AI](https://www.youtube.com/@mu_li)
- [Transformer](https://www.youtube.com/watch?v=ugWDIIOHtPA&list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&index=61)
- [Transformer模型详解（图解最完整版）](https://zhuanlan.zhihu.com/p/338817680)
- [大语言模型集成工具 LangChain](https://zhuanlan.zhihu.com/p/599688026)
- [大模型解惑](https://www.cnblogs.com/pam-sh/p/17852240.html)
- [整理了48个代码大模型分享！涵盖原始、改进、专用、微调4大类](https://zhuanlan.zhihu.com/p/669434367)
- [大模型到底能有多“大”？](https://www.thepaper.cn/newsDetail_forward_26274423)
- [GPT-4大模型硬核解读，看完成半个专家](https://36kr.com/p/2196628560234373)
