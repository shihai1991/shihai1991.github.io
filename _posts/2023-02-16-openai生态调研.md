---
layout: post
title: AI生态调研
category: AI
catalog: true
published: true
tags:
  - AI
time: '2023.02.16 10:18:00'
---
> 最近貌似全民都在关注ChatGPT，当前这个领域的生态发展看起来挺快的，对这块没有太多概念，借机学学。

> 所有内容、图片均摘抄自参考文献内容。


# 一、若干生态技术
## 1.1 [openAI GPT系列](https://github.com/openai)
- 2018年：OpenAI发布最初版本的GPT模型，从 OpenAI 公开的论文（Improving Language Understanding by Generative Pre-Training）可以了解到，这个模型采用了12层的Transformer Decoder结构，用了大约5GB的无监督文本数据进行语言模型任务的训练。虽然**初代GPT模型采用的就已经是生成式的预训练**（这也是 GPT 名字的由来，Generative Pre-Training，即生成式预训练），但使用的是无监督预训练+下游任务微调的范式。
- 2019年：OpenAI发布了有48层Transformer结构的[GPT-2模型](https://github.com/openai/gpt-2)。在发布的论文（Language Models are Unsupervised Multitask Learners）中，他们发现通过无监督数据配合生成式训练后，GPT 展示出了零样本（zero-shot）的多任务能力。
- 2020年：OpenAI发布了1750亿参数量的GPT-3，一个即便以现在的眼光去看也大得让人惊叹的模型。虽然OpenAI没有明确公开训练这个模型的费用，但大家的估算是当时花了 1200 万美元。同时公开的还有一篇长达60多页的论文（Language Models are Few-Shot Learners）。
- 2021年：OpenAI 在对 GPT-3 的研究中还有一个意外的发现，它能够根据一些注释生成很简单的代码。因此在随后的 2021 年，他们对生成代码这件事情进行了专门的研究投入，并发布了 CodeX 模型。它可以看作是一个有着代码专精能力的 GPT 模型，能够根据自然语言输入生成比较复杂的代码。
- 2022年：OpenAI 发表了 InstructGPT 的论文（Training language models to follow instructions with human feedback），从中我们可以一窥解决「一本正经地胡说八道」和「输出有害的内容」这样问题的方法。**InstructGPT 提出了两个阶段的路径来让 GPT 学习人类给出的「优秀范例」，第一阶段是监督学习，第二阶段是强化学习。**
![]({{site.baseurl}}/img/2023/Q2/20230410-GPT.jpg)

## 1.1.1 [DocsGPT](https://github.com/arc53/DocsGPT)
帮助大家能更加高效的在项目文档中找到价值信息。

## 1.2 [ChatGLM](https://chatglm.cn/blog)
模型回答：由清华大学 KEG 实验室与智谱AI共同训练的大型语言模型，名为“ChatGLM”。ChatGLM 是由清华大学 KEG 实验室与智谱AI共同开发而成。  
有两个开源版本：ChatGLM-6B，ChatGLM-130B。

## 1.3 [nanoGPT](https://github.com/karpathy/nanoGPT)
由前特斯拉AI总监、李飞飞徒弟-Andrej Karpathy创建的

## 1.4 [LLaMA](https://github.com/facebookresearch/llama)
meta开源的ChatGPT模型。

# 二、相关其他服务
## 2.1 [Amazon SageMaker](https://catalog.us-east-1.prod.workshops.aws/workshops/1ac668b1-dbd3-4b45-bf0a-5bc36138fcf1/zh-CN)
SageMaker非常明智的把注意力放到**训练模型**和**发布模型**上， 让数据科学家去做要针对的编程工作。

## 2.2 [Hugging Face](https://huggingface.co/)
Hugging face 起初是一家总部位于纽约的聊天机器人初创服务商，他们本来打算创业做聊天机器人，然后在github上开源了一个Transformers库，虽然聊天机器人业务没搞起来，但是他们的这个库在机器学习社区迅速大火起来。目前已经共享了超100,000个预训练模型，10,000个数据集，变成了机器学习界的github。

## 2.3 [langchain](https://langchain.readthedocs.io/en/latest/)

## 2.4 [chatterbot](https://chatterbot.readthedocs.io/)


# 三、AI执行逻辑
TODO

# 四、参考文献
- [ChatGPT的前世今生：OpenAI的技术「执拗」与「豪赌」](https://www.8btc.com/article/6805740)
- [热点解读：大模型的突现能力和ChatGPT引爆的范式转变](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650864784&idx=3&sn=fb8ad092ad32623af7ea822652cd14cc&chksm=84e538eeb392b1f8ff40fa10dc2c84b56904fed261ff15f97f36a1023f887807af62ea39bde7&scene=21#wechat_redirect)
- [ChatGPT的各项超能力从哪儿来？万字拆解追溯技术路线图来了！](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650864144&idx=4&sn=1270624988d70f44d4059af7ac4ae4e0&chksm=84e53e6eb392b7785418e8257952284cfe6dd801d84404958fb917c461da792039626e172c31&scene=21#wechat_redirect)
- [ChatGLM](https://chatglm.cn/blog)
- [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)
- [如何看待亚马逊最新发布的 SageMaker？](https://www.zhihu.com/question/263394266)
- [Huggingface 超详细介绍](https://zhuanlan.zhihu.com/p/535100411)
- [李飞飞高徒教你从0到1构建GPT，马斯克点赞](https://zhuanlan.zhihu.com/p/600057039)
- [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)
