---
layout: post
title: 2025年AI工具总结
category: AI
catalog: true
published: false
tags:
  - AI
time: '2025.12.20 12:58:00'
---

# 2025年研发用AI工具全景指南
研发用AI工具已覆盖代码开发、科研分析、模型构建、安全合规等全流程，主流工具按场景分类如下，便于快速选型。

---

## 一、代码开发核心工具
这类工具聚焦编码全流程，从生成、补全到调试、重构，大幅提升开发效率。

| 工具名称 | 核心功能 | 适用场景 | 关键优势 |
| ---- | ---- | ---- | ---- |
| GitHub Copilot | 实时代码补全、自然语言转代码、单元测试生成 | 全栈开发、快速原型 | 基于GPT-4，支持50+语言，IDE深度集成 |
| Amazon CodeWhisperer | 代码生成、安全扫描、云服务适配 | 云原生开发、AWS生态 | 免费基础版，深度对接AWS SDK，漏洞检测 |
| 通义灵码 | 代码续写、智能纠错、注释生成 | 阿里云生态、企业级开发 | 适配阿里技术栈，支持私有部署 |
| Cursor | 跨文件重构、代码优化、AI对话式开发 | 大型项目重构、复杂逻辑开发 | 基于VS Code，专注AI交互，跨文件分析，国内目前已无法注册使用，可以使用阿里的Qoder |
| Claude Code | 自动解析全库，无需手动传文件 | 从编码到提交部署全链路 | 以命令行交互为核心，可深度理解完整代码库并跨文件协作开发，同时也支持 VS Code 与 JetBrains IDE 集成，聚焦全流程开发效率提升 |
| Tabnine | 本地代码补全、私有库适配 | 企业隐私项目、安全合规场景 | 本地执行，数据不泄露，支持私有训练 |

---

## 二、科研与文献分析工具
面向学术与研究场景，解决文献检索、数据分析、写作润色等痛点。

| 工具名称 | 核心功能 | 适用场景 | 关键优势 |
| ---- | ---- | ---- | ---- |
| ChatGPT / Claude | 长文本分析、文献解读、实验设计辅助 | 多学科研究、复杂逻辑梳理 | Claude支持100K+上下文，擅长流程图生成 |
| DeepSeek-R1 | 文献深度研读、多文件对比、学术写作 | 论文撰写、文献综述 | 128K上下文，支持50个文件上传，免费使用 |
| SciSpace | 文献解析、术语解释、图表理解 | 论文阅读、跨学科研究 | 支持PDF解析，自动生成要点与参考文献 |
| NotebookLM | 文献管理、知识图谱构建、协作标注 | 团队文献协作、研究项目管理 | Google生态，支持多人实时编辑 |

---

## 三、AI模型与应用构建工具
用于快速搭建、训练和部署AI模型/应用，降低开发门槛。

| 工具名称 | 核心功能 | 适用场景 | 关键优势 |
| ---- | ---- | ---- | ---- |
| LangChain | LLM应用开发框架、多工具集成 | 定制化AI系统、对话机器人 | 模块化设计，支持复杂任务链 |
| LlamaIndex | 知识库构建、检索增强生成（RAG） | 企业知识库、问答系统 | 高效数据接入与索引管理 |
| JetBrains AI Assistant | IDE原生AI集成、代码理解、调试 | Java/Python等IDE内开发 | 与IntelliJ系IDE深度融合，本地优先 |
| DeepSeek-Coder | 代码生成、模型微调、本地部署 | 开源项目、低成本开发 | 国产开源，支持本地部署，中文友好 |

---

## 四、代码质量与安全工具
聚焦代码安全、性能优化与合规检测，保障研发产出质量。

| 工具名称 | 核心功能 | 适用场景 | 关键优势 |
| ---- | ---- | ---- | ---- |
| Snyk | 漏洞扫描、依赖管理、合规检测 | 企业级项目、开源组件安全 | 实时检测，支持修复建议自动生成 |
| Codiga | 代码质量分析、规范检查、重构建议 | 团队协作、代码评审 | 支持自定义规则，集成CI/CD |
| CodeGPT | 代码逻辑分析、安全审计、注释生成 | 代码评审、遗留系统维护 | 集成GPT模型，可解释复杂代码 |

---

## 五、项目协作与文档工具
提升团队协作效率，自动化文档生成与知识管理。

| 工具名称 | 核心功能 | 适用场景 | 关键优势 |
| ---- | ---- | ---- | ---- |
| Mintlify | 自动生成API文档、代码注释 | API开发、文档维护 | 支持多语言，与代码库同步更新 |
| Swimm AI | 代码库知识沉淀、协作文档生成 | 新成员上手、团队知识共享 | 结合代码上下文，自动生成教程 |
| Otter.ai | 会议录音转写、任务提取、协作标注 | 研发会议、需求评审 | 实时转写，支持多人协作编辑 |

---

## 智能体通信协议
智能体通信有三种通信协议：
- MCP（Model Context Protocol）用于智能体与工具的标准化通信；
- A2A（Agent-to-Agent Protocol）用于智能体间的点对点协作；
- ANP（Agent Network Protocol）用于构建大规模智能体网络。这三种协议共同构成了智能体通信的基础设施层；

### MCP
一个基于fastmcp的mcp示例。
```python
# 1. 导入 FastMCP 核心组件与 OpenAI 依赖
from fastmcp import FastMCP, Tool
import openai
import os

# 配置 OpenAI API 密钥（替换为你的实际密钥）
os.environ["OPENAI_API_KEY"] = "你的OpenAI API密钥"
client = openai.OpenAI()

# 2. 定义自定义工具：加法计算器（FastMCP 核心是 Tool 类封装工具）
def add_tool(a: float, b: float) -> float:
    """
    两数相加的计算器工具
    :param a: 第一个数字
    :param b: 第二个数字
    :return: 两数之和
    """
    return a + b

# 将工具封装为 FastMCP 可识别的 Tool 对象
addition_tool = Tool(
    name="addition_calculator",  # 工具唯一名称（LLM 会通过该名称调用）
    func=add_tool,               # 工具对应的执行函数
    description="用于计算两个数字的和，接收两个浮点型参数 a 和 b，返回它们的相加结果"  # 工具描述（关键！LLM 依赖描述判断是否调用该工具）
)

# 3. 创建 FastMCP 实例，并注册工具
mcp = FastMCP()
mcp.register_tool(addition_tool)  # 注册单个工具（也可通过 register_tools 批量注册）

# 4. 核心流程：LLM 生成工具调用指令 → FastMCP 执行工具 → 返回结果给 LLM
def llm_call_tool_with_fastmcp(question: str):
    # 第一步：获取 LLM 的工具调用意图（通过 tools 参数传递 FastMCP 注册的工具信息）
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": question}],
        tools=mcp.get_tools_schema(),  # FastMCP 自动生成工具 schema（符合 OpenAI Tool Calling 规范）
        tool_choice="auto"  # 让 LLM 自动判断是否需要调用工具
    )

    # 第二步：提取 LLM 的工具调用参数，并通过 FastMCP 执行工具
    tool_calls = response.choices[0].message.tool_calls
    if tool_calls:
        # FastMCP 自动解析工具调用参数并执行，返回工具执行结果
        tool_results = mcp.run_tool_calls(tool_calls)
        
        # 第三步：将工具执行结果返回给 LLM，生成最终回答
        final_response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "user", "content": question},
                response.choices[0].message,  # LLM 之前的工具调用指令
                {"role": "tool", "content": str(tool_results)}  # 工具执行结果
            ]
        )
        return final_response.choices[0].message.content
    else:
        # 无需调用工具，直接返回 LLM 回答
        return response.choices[0].message.content

# 5. 运行示例
if __name__ == "__main__":
    # 提问：需要调用加法工具的问题
    user_question = "请帮我计算 123.45 + 678.9 的结果是多少？"
    print(f"用户问题：{user_question}")
    print("-" * 50)
    
    # 执行 LLM + FastMCP 工具调用流程
    result = llm_call_tool_with_fastmcp(user_question)
    print(f"最终回答：{result}")
```

[OpenAI MCP](https://platform.openai.com/docs/guides/tools-connectors-mcp)  
[FastMCP OpenAI](https://gofastmcp.com/integrations/openai)  
Ollama 0.13.3版本以后才兼容支持openAI的`v1/responses`接口。[链接](https://docs.ollama.com/api/openai-compatibility#%2Fv1%2Fresponses)
